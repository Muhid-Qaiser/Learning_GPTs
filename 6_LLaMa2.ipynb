{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db36556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb44a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dims: int = 4096\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256\n",
    "    ff_dim_multipler: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    # Needed for Kv cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args:ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # Model parameters\n",
    "        assert args.vocab_size != -1, \"Vocab_size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.tok_embedddings = nn.Embedding(args.vocab_size, args.dims)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dims, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dims, args.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.args.dims // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int, use_kv_cache: bool = False, ):\n",
    "        batch_size, seq_len = tokens.shape\n",
    "\n",
    "        if use_kv_cache:\n",
    "            assert seq_len == 1, \"When using kv cache, the sequence length must be 1\"\n",
    "\n",
    "            h = self.tok_embedddings(tokens)\n",
    "\n",
    "            freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len].to(h.device)\n",
    "\n",
    "\n",
    "            for layer in self.layers:\n",
    "                h = layer(h, start_pos, freqs_complex)\n",
    "            h = self.norm(h)\n",
    "            logits = self.output(h)\n",
    "            return logits, None, None             \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc9bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.,  -4.,  -8., -12., -16., -20., -24., -28., -32., -36.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 20, 2).float() * -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09645d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08097025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c7f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
